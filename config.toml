# LLM Provider Configuration
# Choose which LLM provider to use: "gemini" or "azure"
[provider]
default = "gemini"  # Options: "gemini" or "azure"

# Rate limiting configuration (requests per minute)
[rate_limits]
gemini = 15  # Gemini AI Studio API: increased from 8 to 15 RPM (no 429 errors observed)
azure = 50   # Azure S0 tier: limited by tokens, but ~50 requests/minute safe estimate

# Gemini AI Configuration
# This file contains settings for Google Generative AI integration

[model]
# Primary model for Turkish text generation
name = "gemini-2.5-pro"  # Higher quality than Flash, should have fresh daily quota

# Alternative models (uncomment to use)
# name = "gemini-2.0-flash-exp"  # Hit quota: 504/500 RPD
# name = "gemini-2.0-flash"      # Stable 2.0 version, 1,500 RPD
# name = "gemini-flash-latest"   # Always points to latest flash model
# name = "gemini-1.5-flash"      # NOT AVAILABLE via AI Studio API

[generation]
# Text generation parameters
temperature = 0.7
max_output_tokens = 8192
top_p = 0.8
top_k = 40

[DIAL_API]
DIAL_API_ENDPOINT="https://ai-proxy.lab.epam.com/openai/deployments/{model_id}/chat/completions"
# "https://dklmnopenai.services.ai.azure.com"
OPENAI_API_VERSION = "2024-08-01-preview"
OPENAI_MODEL_NAME = "gpt-4o-2024-11-20"  # GPT-4o - better quality than mini but heavily rate limited
GPT41_MODEL_NAME = "gpt-4.1-2025-04-14"
GPT5_MODEL_NAME = "gpt-5-2025-08-07"
CLOUD_MODEL_NAME = "claude-haiku-4-5@20251001"
DEEP_SEEK_MODEL_NAME = "deepseek-r1"
GEMINI_MODEL_NAME = "gemini-2.5-flash"
LLAMA_MODEL_NAME = "meta.llama4-maverick-17b-instruct-v1:0"

# Claude model rotation list - will try each in order when hitting daily limits
# Each model has its own daily quota, so we can rotate through them
CLAUDE_MODEL_ROTATION = [
    "claude-sonnet-4-5@20250929",    # Claude 4.5 Sonnet - AVAILABLE (not the anthropic.* version which is blocked)
    "gemini-2.5-flash",              # Gemini fallback
]

# Default provider for create_training_example.py pipeline
# Options: openai, claude, gemini, deepseek, llama
DEFAULT_PROVIDER = "llama"  # Using Llama Scout - 5M tokens/day

