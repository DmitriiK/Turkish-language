# LLM Provider Configuration
# Choose which LLM provider to use: "gemini" or "azure"
[provider]
default = "gemini"  # Options: "gemini" or "azure"

# Rate limiting configuration (requests per minute)
[rate_limits]
gemini = 15  # Gemini AI Studio API: increased from 8 to 15 RPM (no 429 errors observed)
azure = 50   # Azure S0 tier: limited by tokens, but ~50 requests/minute safe estimate

# Gemini AI Configuration
# This file contains settings for Google Generative AI integration

[model]
# Primary model for Turkish text generation
name = "gemini-2.5-flash"  # Stable model with 10K RPD quota (9,594 remaining)

# Alternative models (uncomment to use)
# name = "gemini-2.0-flash-exp"  # Hit quota: 504/500 RPD
# name = "gemini-2.0-flash"      # Stable 2.0 version, 1,500 RPD
# name = "gemini-flash-latest"   # Always points to latest flash model
# name = "gemini-1.5-flash"      # NOT AVAILABLE via AI Studio API

[generation]
# Text generation parameters
temperature = 0.7
max_output_tokens = 8192
top_p = 0.8
top_k = 40

[DIAL_API]
DIAL_API_ENDPOINT="https://ai-proxy.lab.epam.com/openai/deployments/{model_id}/chat/completions"
# "https://dklmnopenai.services.ai.azure.com"
OPENAI_API_VERSION = "2024-08-01-preview"
OPENAI_MODEL_NAME = "gpt-4o-mini-2024-07-18"
CLOUD_MODEL_NAME = "anthropic.claude-haiku-4-5-20251001-v1:0"
DEEP_SEEK_MODEL_NAME = "deepseek-r1"
GEMINI_MODEL_NAME = "gemini-2.5-flash"

# Default provider for create_training_example.py pipeline
# Options: openai, claude, gemini, deepseek
default_provider = "openai"

